{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import hmean\n",
    "import math\n",
    "import seaborn as sn\n",
    "import random\n",
    "from scipy import ndarray\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import smote_variants as sv\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = ['paleturquoise','c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(arr):\n",
    "    'Summarizing final results'\n",
    "    x=np.mean(np.asarray(arr), axis = 0)\n",
    "    y=np.std(np.asarray(arr), axis = 0)\n",
    "    return(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b,seed_perm):\n",
    "    'Shuffling the feature matrix along with the labels with same order'\n",
    "    np.random.seed(seed_perm)##change seed 1,2,3,4,5\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OVS(training_data,training_labels,neb):\n",
    "    'Generating oversampled data for several models using smote_variants library'\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    smote_polynom_fit_SMOTE = sv.polynom_fit_SMOTE(random_state=seed_perm)\n",
    "    SMOTE_feat_polynom_fit_SMOTE, SMOTE_labels_polynom_fit_SMOTE = smote_polynom_fit_SMOTE.sample(training_data, training_labels)\n",
    "    \n",
    "    smote_ProWSyn = sv.ProWSyn(random_state=seed_perm, n_neighbors=neb )\n",
    "    SMOTE_feat_ProWSyn, SMOTE_labels_ProWSyn = smote_ProWSyn.sample(training_data, training_labels)\n",
    "    \n",
    "    smote_cure = sv.CURE_SMOTE(random_state=seed_perm, )\n",
    "    SMOTE_feat_cure, SMOTE_labels_cure = smote_cure.sample(training_data, training_labels)\n",
    "    \n",
    "    smote_SOMO = sv.SOMO(random_state=seed_perm)\n",
    "    SMOTE_feat_SOMO, SMOTE_labels_SOMO = smote_SOMO.sample(training_data, training_labels)\n",
    "    \n",
    "    sm = sv.SMOTE(random_state=seed_perm, n_neighbors=neb)\n",
    "    SMOTE_feat, SMOTE_labels = sm.fit_resample(training_data,training_labels)\n",
    "    \n",
    "    \n",
    "    return(SMOTE_feat, SMOTE_labels, SMOTE_feat_polynom_fit_SMOTE, SMOTE_labels_polynom_fit_SMOTE, SMOTE_feat_ProWSyn, SMOTE_labels_ProWSyn,\\\n",
    "          SMOTE_feat_cure, SMOTE_labels_cure, SMOTE_feat_SOMO, SMOTE_labels_SOMO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch on the classifier you want to use (press ctrl+y on the cell). For every classifier, for every dataset we have provided the oversampling scheme we have used. Use the same parameter values to exactly reproduce the results:\n",
    "- High Gloval Variance(HGV) => neb_conv=1000, max_conv=2 \n",
    "- High Lcal Vriance(HLV) => neb_conv=5, max_conv=2 \n",
    "- Low Goval Vriance(LGV) => neb_conv=1000, max_conv=features_x.shape[1]\n",
    "- Low Local Variance(LLV) => neb_conv=5, max_conv=features_x.shape[1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def classifier(X_train,y_train,X_test,y_test):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    from sklearn import metrics\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import f1_score\n",
    "    from scipy.stats import hmean\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    from sklearn.metrics import balanced_accuracy_score\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "    gb = GradientBoostingClassifier(random_state=0)\n",
    "    gb.fit(X_train, y_train)\n",
    "    y_pred= gb.predict(X_test)\n",
    "    y_proba=gb.predict_proba(X_test)[:, 1]\n",
    "    con_mat=confusion_matrix(y_test,y_pred)\n",
    "    aps=average_precision_score(y_test,y_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    kappa=cohen_kappa_score(y_test, y_pred)\n",
    "    return(f1, kappa, aps, con_mat)\n",
    "classify='Grad_Boost'\n",
    "sigma_model=0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|dataset|strategy|\n",
    "|:-|---|\n",
    "|abalone9-18|HGV|\n",
    "|abalone_17_vs_7_8_9_10|HGV|\n",
    "|car-vgood|LLV|\n",
    "|car_good|LGV|\n",
    "|flare_F|HGV|\n",
    "|hypothyroid|LGV|\n",
    "|kddcup-guess_passwd_vs_satan|HLV|\n",
    "|kr-vs-k-three_vs_eleven|LGV|\n",
    "|kr-vs-k-zero-one_vs_draw|LGV|\n",
    "|oil|HGV|\n",
    "|ozone_level|HGV|\n",
    "|shuttle-2_vs_5|HGV|\n",
    "|solar_flare_m0|HLV|\n",
    "|thyroid_sick|LGV|\n",
    "|wine_quality|HLV|\n",
    "|winequality-red-4|HLV|\n",
    "|yeast4|HLV|\n",
    "|yeast5|HLV|\n",
    "|yeast6|LGV|\n",
    "|yeast_me2|LLV|"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def classifier(X_train,y_train,X_test,y_test):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    from sklearn import metrics\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    from scipy.stats import hmean\n",
    "    from sklearn.metrics import balanced_accuracy_score\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "    y_pred= logreg.predict(X_test)\n",
    "    y_proba=logreg.predict_proba(X_test)[:, 1]\n",
    "    con_mat=confusion_matrix(y_test,y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    kappa=cohen_kappa_score(y_test, y_pred)\n",
    "    aps=average_precision_score(y_test,y_proba)\n",
    "    return(f1, kappa, aps, con_mat)\n",
    "classify='Log_Reg'\n",
    "sigma_model=0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|dataset|strategy|\n",
    "|:-|---|\n",
    "|abalone9-18|HGV|\n",
    "|abalone_17_vs_7_8_9_10|LLV|\n",
    "|car-vgood|LGV|\n",
    "|car_good|LGV|\n",
    "|flare_F|LGV|\n",
    "|hypothyroid|LGV|\n",
    "|kddcup-guess_passwd_vs_satan|LGV|\n",
    "|kr-vs-k-three_vs_eleven|LGV|\n",
    "|kr-vs-k-zero-one_vs_draw|LGV|\n",
    "|oil|LGV|\n",
    "|ozone_level|LGV|\n",
    "|shuttle-2_vs_5|LGV|\n",
    "|solar_flare_m0|LGV|\n",
    "|thyroid_sick|LLV|\n",
    "|wine_quality|LGV|\n",
    "|winequality-red-4|LGV|\n",
    "|yeast4|LGV|\n",
    "|yeast5|LLV|\n",
    "|yeast6|LLV|\n",
    "|yeast_me2|LGV|"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def classifier(X_train,y_train,X_test,y_test):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    from sklearn import metrics\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import f1_score\n",
    "    from scipy.stats import hmean\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    from sklearn.metrics import balanced_accuracy_score\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "    gb = RandomForestClassifier(random_state=0)\n",
    "    gb.fit(X_train, y_train)\n",
    "    y_pred= gb.predict(X_test)\n",
    "    y_proba=gb.predict_proba(X_test)[:, 1]\n",
    "    con_mat=confusion_matrix(y_test,y_pred)\n",
    "    aps=average_precision_score(y_test,y_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    kappa=cohen_kappa_score(y_test, y_pred)\n",
    "    return(f1, kappa, aps, con_mat)\n",
    "classify='Rand_For'\n",
    "sigma_model=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|dataset|strategy|\n",
    "|:-|---|\n",
    "|abalone9-18|HGV|\n",
    "|abalone_17_vs_7_8_9_10|HGV|\n",
    "|car-vgood|HLV|\n",
    "|car_good|HLV|\n",
    "|flare_F|HGV|\n",
    "|hypothyroid|LGV|\n",
    "|kddcup-guess_passwd_vs_satan|HGV|\n",
    "|kr-vs-k-three_vs_eleven|HGV|\n",
    "|kr-vs-k-zero-one_vs_draw|LGV|\n",
    "|oil|HGV|\n",
    "|ozone_level|HGV|\n",
    "|shuttle-2_vs_5|HGV|\n",
    "|solar_flare_m0|HLV|\n",
    "|thyroid_sick|HLV|\n",
    "|wine_quality|HLV|\n",
    "|winequality-red-4|HGV|\n",
    "|yeast4|HGV|\n",
    "|yeast5|HGV|\n",
    "|yeast6|HLV|\n",
    "|yeast_me2|HLV|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(X_train,y_train,X_test,y_test):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    from sklearn import metrics\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import f1_score\n",
    "    from scipy.stats import hmean\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    from sklearn.metrics import balanced_accuracy_score\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred= knn.predict(X_test)\n",
    "    y_proba=knn.predict_proba(X_test)[:, 1]\n",
    "    con_mat=confusion_matrix(y_test,y_pred)\n",
    "    aps=average_precision_score(y_test,y_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    kappa=cohen_kappa_score(y_test, y_pred)\n",
    "    return(f1, kappa, aps, con_mat)\n",
    "classify='K_NN'\n",
    "sigma_model=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|dataset|strategy|\n",
    "|:-|---|\n",
    "|abalone9-18|LLV|\n",
    "|abalone_17_vs_7_8_9_10|LLV|\n",
    "|car-vgood|LLV|\n",
    "|car_good|LLV|\n",
    "|flare_F|LLV|\n",
    "|hypothyroid|LLV|\n",
    "|kddcup-guess_passwd_vs_satan|LLV|\n",
    "|kr-vs-k-three_vs_eleven|LLV|\n",
    "|kr-vs-k-zero-one_vs_draw|LLV|\n",
    "|oil|LLV|\n",
    "|ozone_level|LLV|\n",
    "|shuttle-2_vs_5|LLV|\n",
    "|solar_flare_m0|LLV|\n",
    "|thyroid_sick|LLV|\n",
    "|wine_quality|LLV|\n",
    "|winequality-red-4|LLV|\n",
    "|yeast4|LLV|\n",
    "|yeast5|LLV|\n",
    "|yeast6|LLV|\n",
    "|yeast_me2|LLV|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining ProWRAS function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Neb_grps(data,neb_conv,n_jobs):\n",
    "    'Function calculating nearest neb_conv neighbours (among input data points), for every input data point'\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=neb_conv, n_jobs=n_jobs).fit(data)\n",
    "    distances, indices = nbrs.kneighbors(data)\n",
    "    neb_class=[]\n",
    "    for i in (indices):\n",
    "        neb_class.append(i)\n",
    "    return(np.asarray(neb_class)) \n",
    "\n",
    "def partition_info(X, y, max_levels, n_neighbours_max, theta, n_jobs):\n",
    "    \n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        \n",
    "        features_1_trn=X[np.where(y==1)]\n",
    "        features_0_trn=X[np.where(y!=1)]\n",
    "        \n",
    "        \n",
    "        L=max_levels\n",
    "\n",
    "        # Step 2\n",
    "        P = np.where(y == 1)[0]\n",
    "        X_maj = features_0_trn\n",
    "\n",
    "        Ps = []\n",
    "        proximity_levels = []\n",
    "        \n",
    "\n",
    "        # Step 3\n",
    "        for i in range(1,L):\n",
    "            if len(P) == 0:\n",
    "                break\n",
    "            # Step 3 a\n",
    "            n_neighbours_max = min([len(P),n_neighbours_max])\n",
    "            nn = NearestNeighbors(n_neighbors=n_neighbours_max, n_jobs=n_jobs)\n",
    "            nn.fit(X[P])\n",
    "            distances, indices = nn.kneighbors(X_maj)\n",
    "\n",
    "            # Step 3 b\n",
    "            P_i = np.unique(np.hstack([i for i in indices]))\n",
    "\n",
    "            # Step 3 c - proximity levels are encoded in the Ps list index\n",
    "            Ps.append(P[P_i])\n",
    "            proximity_levels.append(i+1)\n",
    "\n",
    "            # Step 3 d\n",
    "            P = np.delete(P, P_i)\n",
    "\n",
    "        # Step 4\n",
    "        if len(P) > 0:\n",
    "            Ps.append(P)\n",
    "\n",
    "        # Step 5\n",
    "        if len(P) > 0:\n",
    "            proximity_levels.append(i)\n",
    "            proximity_levels = np.array(proximity_levels)\n",
    "            \n",
    "        \n",
    "        # Step 6\n",
    "        weights = np.array([np.exp(-theta*(proximity_levels[i] - 1))\n",
    "                            for i in range(len(proximity_levels))])\n",
    "        # weights is the probability distribution of sampling in the\n",
    "        # clusters identified\n",
    "        weights = weights/np.sum(weights)\n",
    "        return (np.array(Ps),weights)\n",
    "    \n",
    "def high_global_variance(data,num_samples_to_generate):\n",
    "    np.random.seed(42)\n",
    "    generated_data=[]\n",
    "    for i in range(int(num_samples_to_generate)):\n",
    "        r_1= np.random.randint(len(data))\n",
    "        r_2= np.random.randint(len(data))\n",
    "        wts=np.array([np.random.randint(1,100),np.random.randint(1,100)])\n",
    "        aff_w=wts/sum(wts)\n",
    "        data_tsl=data[np.array([r_1,r_2])]\n",
    "        sample=np.dot(aff_w, data_tsl)\n",
    "        generated_data.append(sample)\n",
    "    return(np.array(generated_data))\n",
    "\n",
    "\n",
    "\n",
    "def low_global_variance(data,cluster_size,shadow,sigma,num_samples_to_generate,num_convcom):\n",
    "    np.random.seed(42)\n",
    "    data_shadow=([])\n",
    "    for i in range(cluster_size):\n",
    "        c=0\n",
    "        while c<shadow:\n",
    "            data_shadow.append(data[i]+np.random.normal(0,sigma))\n",
    "            c=c+1\n",
    "    data_shadow=np.array(data_shadow)\n",
    "    \n",
    "    data_shadow_lc=([])\n",
    "\n",
    "    for i in range(int(num_samples_to_generate)):\n",
    "        idx = np.random.randint(int(shadow*cluster_size), size=int(num_convcom))\n",
    "        w=np.random.randint(100, size=len(idx))\n",
    "        aff_w=np.array(w/sum(w))\n",
    "        data_tsl=data_shadow[idx]\n",
    "        data_tsl_=np.dot(aff_w, data_tsl)        \n",
    "        data_shadow_lc.append(data_tsl_)\n",
    "    data_shadow_lc=np.array(data_shadow_lc)\n",
    "    return(data_shadow_lc)   \n",
    "\n",
    "\n",
    "\n",
    "def high_local_variance(data,neb_conv, num_samples_to_generate,n_jobs):\n",
    "    np.random.seed(42)\n",
    "    generated_data=[]\n",
    "    neb_list=Neb_grps(data,neb_conv,n_jobs)\n",
    "    for i in range(int(num_samples_to_generate)):\n",
    "        random_neighbourhood=neb_list[np.random.randint(len(neb_list))]\n",
    "        r_1= np.random.randint(neb_conv)\n",
    "        r_2= np.random.randint(neb_conv)\n",
    "        wts=np.array([np.random.randint(1,100),np.random.randint(1,100)])\n",
    "        aff_w=wts/sum(wts)\n",
    "        data_tsl=np.array([data[random_neighbourhood][r_1],data[random_neighbourhood][r_2]])\n",
    "        sample= np.dot(aff_w, data_tsl)\n",
    "        generated_data.append(sample)\n",
    "    return(np.array(generated_data))\n",
    "\n",
    "\n",
    "\n",
    "def low_local_variance(data, cluster_size, neb_conv, shadow,sigma,num_samples_to_generate,num_convcom,n_jobs):\n",
    "    'Function creating LoRAS samples for one minority data point neighbourhood'\n",
    "    np.random.seed(42)\n",
    "    neb_list=Neb_grps(data,neb_conv,n_jobs)\n",
    "    \n",
    "    \n",
    "    data_shadow_lc=([])\n",
    "    for i in range(int(num_samples_to_generate)):\n",
    "        random_neighbourhood=neb_list[np.random.randint(len(neb_list))]\n",
    "        data_neighbourhood=data[random_neighbourhood]\n",
    "    \n",
    "        data_shadow=([])\n",
    "        \n",
    "        for i in range(neb_conv):\n",
    "            c=0\n",
    "            while c<shadow:\n",
    "                data_shadow.append(data_neighbourhood[i]+np.random.normal(0,sigma))\n",
    "                c=c+1\n",
    "        data_shadow=np.array(data_shadow)\n",
    "        \n",
    "        idx = np.random.randint(int(shadow*neb_conv), size=int(num_convcom))\n",
    "        w=np.random.randint(100, size=len(idx))\n",
    "        aff_w=np.array(w/sum(w))\n",
    "        data_tsl=data_shadow[idx]\n",
    "        data_tsl_=np.dot(aff_w, data_tsl)        \n",
    "        data_shadow_lc.append(data_tsl_)\n",
    "    data_shadow_lc=np.array(data_shadow_lc)\n",
    "    return(data_shadow_lc)   \n",
    "\n",
    "\n",
    "def ProWRAS_gen(data, labels, max_levels, neb_conv, n_neighbours_max, max_conv, num_samples_to_generate, theta, shadow, sigma, n_jobs):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    features_1_trn=data[np.where(labels==1)]\n",
    "    features_0_trn=data[np.where(labels!=1)]\n",
    "    \n",
    "    num_feats=data.shape[1]\n",
    "    clusters, weights = partition_info(data, labels, max_levels, n_neighbours_max, theta, n_jobs)\n",
    " \n",
    "    num_samples_each_cluster=np.ceil(num_samples_to_generate*weights)\n",
    "    num_convcomb_each_cluster=np.ceil((weights/max(weights))*max_conv)\n",
    "    synth_samples=[]\n",
    "\n",
    "    for i in range(len(weights)):\n",
    "\n",
    "        if clusters[i].shape[0]>neb_conv:\n",
    "            \n",
    "            if num_convcomb_each_cluster[i]<num_feats:\n",
    "                print('High Local Variance Generation\\n')\n",
    "                synth=high_local_variance(data[clusters[i]], neb_conv, num_samples_each_cluster[i],n_jobs)\n",
    "                synth_samples.append(synth)\n",
    "            else:\n",
    "                print('Low Local Variance Generation\\n')\n",
    "                synth=low_local_variance(data[clusters[i]],data[clusters[i]].shape[0],neb_conv,shadow,sigma,num_samples_each_cluster[i],num_convcomb_each_cluster[i],n_jobs)\n",
    "                synth_samples.append(synth)\n",
    "        else:\n",
    "            if num_convcomb_each_cluster[i]<num_feats:\n",
    "                print('High Global Variance Generation\\n')\n",
    "                synth=high_global_variance(data[clusters[i]], num_samples_each_cluster[i])\n",
    "                synth_samples.append(synth)\n",
    "            else:\n",
    "                print('Low Global Variance Generation\\n')\n",
    "                synth=low_global_variance(data[clusters[i]],data[clusters[i]].shape[0],shadow,sigma,num_samples_each_cluster[i],num_convcomb_each_cluster[i])\n",
    "                synth_samples.append(synth)\n",
    "                \n",
    "    synth_samples=np.array(synth_samples)\n",
    "    synth_samples=np.concatenate(synth_samples)\n",
    "    \n",
    "    ProWRAS_train=np.concatenate((synth_samples,features_1_trn,features_0_trn))\n",
    "    ProWRAS_labels=np.concatenate((np.ones(len(synth_samples)+len(features_1_trn)),np.zeros(len(features_0_trn))))\n",
    "    return(ProWRAS_train,ProWRAS_labels)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change the dataset name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "switch on the following cell for importing datasets from imblearn.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"wine_quality\" #example\n",
    "from imblearn.datasets import fetch_datasets\n",
    "data = fetch_datasets()[dataset]\n",
    "features_x,labels_x=data.data,data.target\n",
    "\n",
    "n_feat=len(features_x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- switch on the following cell for importing datasets from https://drive.google.com/file/d/1PKw1vETVUzaToomio1-RGzJ9_-buYjOW/view\n",
    "- save the dataset in the same directory as the notebook"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset=\"folding_yeast5\" #example\n",
    "features_x=np.concatenate((data['folding'][0][0],data['folding'][0][2]))\n",
    "labels_x=np.concatenate((data['folding'][0][1],data['folding'][0][3]))\n",
    "n_feat=len(features_x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running 5 x 5 fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting all features and labels for seed:0\n",
      "\n",
      "Dividing data into training and testing datasets for 10-fold CV for seed:0\n",
      "\n",
      "minority class samples:183\n",
      "\n",
      "majority class samples:4715\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 1 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 2 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 3 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 4 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:26:10,045:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 0}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 5 Finished   ############################\n",
      "\n",
      "\n",
      "Using oversampling algorithms to generate and test classification models for seed:0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:26:10,057:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 0}\")\n",
      "2021-01-31 11:26:10,471:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 0}\")\n",
      "2021-01-31 11:26:10,628:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 0}\")\n",
      "2021-01-31 11:26:10,714:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:26:10,715:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 0}\")\n",
      "2021-01-31 11:26:11,758:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 0}\")\n",
      "2021-01-31 11:26:11,770:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 0}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 1 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:26:12,175:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 0}\")\n",
      "2021-01-31 11:26:12,311:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 0}\")\n",
      "2021-01-31 11:26:12,395:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:26:12,396:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 0}\")\n",
      "2021-01-31 11:26:13,358:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 0}\")\n",
      "2021-01-31 11:26:13,371:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 0}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 2 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:26:13,808:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 0}\")\n",
      "2021-01-31 11:26:13,946:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 0}\")\n",
      "2021-01-31 11:26:14,043:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:26:14,043:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 0}\")\n",
      "2021-01-31 11:26:14,990:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 0}\")\n",
      "2021-01-31 11:26:15,001:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 0}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 3 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:26:15,396:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 0}\")\n",
      "2021-01-31 11:26:15,542:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 0}\")\n",
      "2021-01-31 11:26:15,628:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:26:15,629:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 0}\")\n",
      "2021-01-31 11:26:16,576:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 0}\")\n",
      "2021-01-31 11:26:16,589:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 0}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 4 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:26:16,962:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 0}\")\n",
      "2021-01-31 11:26:17,072:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 0}\")\n",
      "2021-01-31 11:26:17,165:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:26:17,166:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 0}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 5 Finished   ############################\n",
      "\n",
      "\n",
      "Using ProWRAS-X to generate and test classification models for seed:0\n",
      "\n",
      "Summing up results for seed:0\n",
      "\n",
      "results ready for seed:0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Extracting all features and labels for seed:1\n",
      "\n",
      "Dividing data into training and testing datasets for 10-fold CV for seed:1\n",
      "\n",
      "minority class samples:183\n",
      "\n",
      "majority class samples:4715\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 1 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 2 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 3 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 4 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:26:40,723:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 1}\")\n",
      "2021-01-31 11:26:40,736:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 1}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 5 Finished   ############################\n",
      "\n",
      "\n",
      "Using oversampling algorithms to generate and test classification models for seed:1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:26:41,098:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 1}\")\n",
      "2021-01-31 11:26:41,243:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 1}\")\n",
      "2021-01-31 11:26:41,339:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:26:41,340:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 1}\")\n",
      "2021-01-31 11:26:42,365:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 1}\")\n",
      "2021-01-31 11:26:42,377:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 1}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 1 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:26:42,733:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 1}\")\n",
      "2021-01-31 11:26:42,848:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 1}\")\n",
      "2021-01-31 11:26:42,946:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:26:42,947:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 1}\")\n",
      "2021-01-31 11:26:43,918:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 1}\")\n",
      "2021-01-31 11:26:43,930:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 1}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 2 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:26:44,333:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 1}\")\n",
      "2021-01-31 11:26:44,443:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 1}\")\n",
      "2021-01-31 11:26:44,524:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:26:44,525:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 1}\")\n",
      "2021-01-31 11:26:45,480:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 1}\")\n",
      "2021-01-31 11:26:45,491:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 1}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 3 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:26:45,895:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 1}\")\n",
      "2021-01-31 11:26:46,034:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 1}\")\n",
      "2021-01-31 11:26:46,125:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:26:46,125:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 1}\")\n",
      "2021-01-31 11:26:47,089:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 1}\")\n",
      "2021-01-31 11:26:47,101:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 1}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 4 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:26:47,471:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 1}\")\n",
      "2021-01-31 11:26:47,596:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 1}\")\n",
      "2021-01-31 11:26:47,674:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:26:47,675:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 1}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 5 Finished   ############################\n",
      "\n",
      "\n",
      "Using ProWRAS-X to generate and test classification models for seed:1\n",
      "\n",
      "Summing up results for seed:1\n",
      "\n",
      "results ready for seed:1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Extracting all features and labels for seed:2\n",
      "\n",
      "Dividing data into training and testing datasets for 10-fold CV for seed:2\n",
      "\n",
      "minority class samples:183\n",
      "\n",
      "majority class samples:4715\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 1 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 2 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 3 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 4 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:27:10,870:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 2}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 5 Finished   ############################\n",
      "\n",
      "\n",
      "Using oversampling algorithms to generate and test classification models for seed:2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:27:10,884:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 2}\")\n",
      "2021-01-31 11:27:11,230:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 2}\")\n",
      "2021-01-31 11:27:11,355:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 2}\")\n",
      "2021-01-31 11:27:11,440:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:27:11,441:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 2}\")\n",
      "2021-01-31 11:27:12,397:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 2}\")\n",
      "2021-01-31 11:27:12,410:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 2}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 1 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:27:12,792:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 2}\")\n",
      "2021-01-31 11:27:12,946:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 2}\")\n",
      "2021-01-31 11:27:13,188:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 2}\")\n",
      "2021-01-31 11:27:14,156:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 2}\")\n",
      "2021-01-31 11:27:14,167:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 2}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 2 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:27:14,586:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 2}\")\n",
      "2021-01-31 11:27:14,716:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 2}\")\n",
      "2021-01-31 11:27:14,812:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:27:14,813:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 2}\")\n",
      "2021-01-31 11:27:15,792:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 2}\")\n",
      "2021-01-31 11:27:15,805:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 2}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 3 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:27:16,170:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 2}\")\n",
      "2021-01-31 11:27:16,293:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 2}\")\n",
      "2021-01-31 11:27:16,373:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:27:16,373:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 2}\")\n",
      "2021-01-31 11:27:17,375:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 2}\")\n",
      "2021-01-31 11:27:17,387:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 2}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 4 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:27:17,803:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 2}\")\n",
      "2021-01-31 11:27:17,949:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 2}\")\n",
      "2021-01-31 11:27:18,031:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:27:18,032:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 2}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 5 Finished   ############################\n",
      "\n",
      "\n",
      "Using ProWRAS-X to generate and test classification models for seed:2\n",
      "\n",
      "Summing up results for seed:2\n",
      "\n",
      "results ready for seed:2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Extracting all features and labels for seed:3\n",
      "\n",
      "Dividing data into training and testing datasets for 10-fold CV for seed:3\n",
      "\n",
      "minority class samples:183\n",
      "\n",
      "majority class samples:4715\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 1 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 2 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 3 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 4 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:27:42,794:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 3}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 5 Finished   ############################\n",
      "\n",
      "\n",
      "Using oversampling algorithms to generate and test classification models for seed:3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:27:42,808:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 3}\")\n",
      "2021-01-31 11:27:43,177:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 3}\")\n",
      "2021-01-31 11:27:43,299:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 3}\")\n",
      "2021-01-31 11:27:43,383:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:27:43,384:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 3}\")\n",
      "2021-01-31 11:27:44,387:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 3}\")\n",
      "2021-01-31 11:27:44,400:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 3}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 1 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:27:44,777:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 3}\")\n",
      "2021-01-31 11:27:44,895:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 3}\")\n",
      "2021-01-31 11:27:44,981:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:27:44,982:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 3}\")\n",
      "2021-01-31 11:27:45,943:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 3}\")\n",
      "2021-01-31 11:27:45,955:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 3}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 2 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:27:46,288:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 3}\")\n",
      "2021-01-31 11:27:46,396:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 3}\")\n",
      "2021-01-31 11:27:46,480:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:27:46,481:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 3}\")\n",
      "2021-01-31 11:27:47,445:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 3}\")\n",
      "2021-01-31 11:27:47,457:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 3}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 3 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:27:47,848:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 3}\")\n",
      "2021-01-31 11:27:47,986:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 3}\")\n",
      "2021-01-31 11:27:48,068:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:27:48,069:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 3}\")\n",
      "2021-01-31 11:27:49,017:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 3}\")\n",
      "2021-01-31 11:27:49,029:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 3}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 4 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:27:49,392:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 3}\")\n",
      "2021-01-31 11:27:49,528:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 3}\")\n",
      "2021-01-31 11:27:49,603:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:27:49,604:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 3}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 5 Finished   ############################\n",
      "\n",
      "\n",
      "Using ProWRAS-X to generate and test classification models for seed:3\n",
      "\n",
      "Summing up results for seed:3\n",
      "\n",
      "results ready for seed:3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Extracting all features and labels for seed:4\n",
      "\n",
      "Dividing data into training and testing datasets for 10-fold CV for seed:4\n",
      "\n",
      "minority class samples:183\n",
      "\n",
      "majority class samples:4715\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 1 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 2 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 3 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n",
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 4 Finished   ############################\n",
      "\n",
      "\n",
      "Running Classifier \n",
      "\n",
      "Low Local Variance Generation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:28:13,445:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 4}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Global Variance Generation\n",
      "\n",
      "\n",
      "\n",
      "############################   Fold 5 Finished   ############################\n",
      "\n",
      "\n",
      "Using oversampling algorithms to generate and test classification models for seed:4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:28:13,459:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 4}\")\n",
      "2021-01-31 11:28:13,816:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 4}\")\n",
      "2021-01-31 11:28:13,953:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 4}\")\n",
      "2021-01-31 11:28:14,036:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:28:14,037:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 4}\")\n",
      "2021-01-31 11:28:14,999:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 4}\")\n",
      "2021-01-31 11:28:15,011:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 4}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 1 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:28:15,382:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 4}\")\n",
      "2021-01-31 11:28:15,506:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 4}\")\n",
      "2021-01-31 11:28:15,599:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:28:15,600:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 4}\")\n",
      "2021-01-31 11:28:16,555:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 4}\")\n",
      "2021-01-31 11:28:16,568:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 4}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 2 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:28:16,951:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 4}\")\n",
      "2021-01-31 11:28:17,098:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 4}\")\n",
      "2021-01-31 11:28:17,177:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:28:17,179:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 4}\")\n",
      "2021-01-31 11:28:18,189:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 4}\")\n",
      "2021-01-31 11:28:18,202:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 4}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 3 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:28:18,558:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 4}\")\n",
      "2021-01-31 11:28:18,701:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 4}\")\n",
      "2021-01-31 11:28:18,786:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:28:18,787:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 4}\")\n",
      "2021-01-31 11:28:19,713:INFO:polynom_fit_SMOTE: Running sampling via ('polynom_fit_SMOTE', \"{'proportion': 1.0, 'topology': 'star', 'random_state': 4}\")\n",
      "2021-01-31 11:28:19,725:INFO:ProWSyn: Running sampling via ('ProWSyn', \"{'proportion': 1.0, 'n_neighbors': 5, 'L': 5, 'theta': 1.0, 'n_jobs': 1, 'random_state': 4}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 4 Finished   ############################\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-31 11:28:20,118:INFO:CURE_SMOTE: Running sampling via ('CURE_SMOTE', \"{'proportion': 1.0, 'n_clusters': 5, 'noise_th': 2, 'n_jobs': 1, 'random_state': 4}\")\n",
      "2021-01-31 11:28:20,247:INFO:SOMO: Running sampling via ('SOMO', \"{'proportion': 1.0, 'n_grid': 10, 'sigma': 0.2, 'learning_rate': 0.5, 'n_iter': 100, 'n_jobs': 1, 'random_state': 4}\")\n",
      "2021-01-31 11:28:20,344:WARNING:SOMO: all clusters filtered\n",
      "2021-01-31 11:28:20,345:INFO:SMOTE: Running sampling via ('SMOTE', \"{'proportion': 1.0, 'n_neighbors': 5, 'n_jobs': 1, 'random_state': 4}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################   Fold 5 Finished   ############################\n",
      "\n",
      "\n",
      "Using ProWRAS-X to generate and test classification models for seed:4\n",
      "\n",
      "Summing up results for seed:4\n",
      "\n",
      "results ready for seed:4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "strata=5\n",
    "for seed_perm in range(strata):\n",
    "    \n",
    "    features_x,labels_x=unison_shuffled_copies(features_x,labels_x,seed_perm)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(features_x)\n",
    "    features_x=(scaler.transform(features_x))\n",
    "    \n",
    "    \n",
    "    ### Extracting all features and labels\n",
    "    print('Extracting all features and labels for seed:'+ str(seed_perm)+'\\n')\n",
    "    \n",
    "    ## Dividing data into training and testing datasets for 10-fold CV\n",
    "    print('Dividing data into training and testing datasets for 10-fold CV for seed:'+ str(seed_perm)+'\\n')\n",
    "    label_1=np.where(labels_x == 1)[0]\n",
    "    label_1=list(label_1)\n",
    "    print('minority class samples:'+ str(len(label_1))+'\\n')\n",
    "\n",
    "    features_1=features_x[label_1]\n",
    "    \n",
    "    neb=5\n",
    "\n",
    "\n",
    "    label_0=np.where(labels_x != 1)[0]\n",
    "    label_0=list(label_0)\n",
    "    len(label_0)\n",
    "\n",
    "    print('majority class samples:'+ str(len(label_0))+'\\n')\n",
    "\n",
    "\n",
    "    features_0=features_x[label_0]\n",
    "    \n",
    "    a=len(features_1)//5\n",
    "    b=len(features_0)//5\n",
    "\n",
    "    fold_1_min=features_1[0:a]\n",
    "    fold_1_maj=features_0[0:b]\n",
    "    fold_1_tst=np.concatenate((fold_1_min,fold_1_maj))\n",
    "    lab_1_tst=np.concatenate((np.zeros(len(fold_1_min))+1, np.zeros(len(fold_1_maj))))\n",
    "\n",
    "    fold_2_min=features_1[a:2*a]\n",
    "    fold_2_maj=features_0[b:2*b]\n",
    "    fold_2_tst=np.concatenate((fold_2_min,fold_2_maj))\n",
    "    lab_2_tst=np.concatenate((np.zeros(len(fold_1_min))+1, np.zeros(len(fold_1_maj))))\n",
    "\n",
    "    fold_3_min=features_1[2*a:3*a]\n",
    "    fold_3_maj=features_0[2*b:3*b]\n",
    "    fold_3_tst=np.concatenate((fold_3_min,fold_3_maj))\n",
    "    lab_3_tst=np.concatenate((np.zeros(len(fold_1_min))+1, np.zeros(len(fold_1_maj))))\n",
    "\n",
    "    fold_4_min=features_1[3*a:4*a]\n",
    "    fold_4_maj=features_0[3*b:4*b]\n",
    "    fold_4_tst=np.concatenate((fold_4_min,fold_4_maj))\n",
    "    lab_4_tst=np.concatenate((np.zeros(len(fold_1_min))+1, np.zeros(len(fold_1_maj))))\n",
    "\n",
    "\n",
    "    fold_5_min=features_1[4*a:]\n",
    "    fold_5_maj=features_0[4*b:]\n",
    "    fold_5_tst=np.concatenate((fold_5_min,fold_5_maj))\n",
    "    lab_5_tst=np.concatenate((np.zeros(len(fold_5_min))+1, np.zeros(len(fold_5_maj))))\n",
    "\n",
    "    fold_1_trn=np.concatenate((fold_2_min,fold_3_min,fold_4_min,fold_5_min, fold_2_maj,fold_3_maj,fold_4_maj,fold_5_maj))\n",
    "\n",
    "    lab_1_trn=np.concatenate((np.zeros(3*a+len(fold_5_min))+1,np.zeros(3*b+len(fold_5_maj))))\n",
    "\n",
    "    fold_2_trn=np.concatenate((fold_1_min,fold_3_min,fold_4_min,fold_5_min,fold_1_maj,fold_3_maj,fold_4_maj,fold_5_maj))\n",
    "\n",
    "    lab_2_trn=np.concatenate((np.zeros(3*a+len(fold_5_min))+1,np.zeros(3*b+len(fold_5_maj))))\n",
    "\n",
    "    fold_3_trn=np.concatenate((fold_2_min,fold_1_min,fold_4_min,fold_5_min,fold_2_maj,fold_1_maj,fold_4_maj,fold_5_maj))\n",
    "\n",
    "    lab_3_trn=np.concatenate((np.zeros(3*a+len(fold_5_min))+1,np.zeros(3*b+len(fold_5_maj))))\n",
    "\n",
    "    fold_4_trn=np.concatenate((fold_2_min,fold_3_min,fold_1_min,fold_5_min,fold_2_maj,fold_3_maj,fold_1_maj,fold_5_maj))\n",
    "\n",
    "    lab_4_trn=np.concatenate((np.zeros(3*a+len(fold_5_min))+1,np.zeros(3*b+len(fold_5_maj))))\n",
    "\n",
    "    fold_5_trn=np.concatenate((fold_2_min,fold_3_min,fold_4_min,fold_1_min,fold_2_maj,fold_3_maj,fold_4_maj,fold_1_maj))\n",
    "\n",
    "    lab_5_trn=np.concatenate((np.zeros(4*a)+1,np.zeros(4*b)))\n",
    "\n",
    "\n",
    "    training_folds_feats=[fold_1_trn,fold_2_trn,fold_3_trn,fold_4_trn,fold_5_trn]\n",
    "\n",
    "    testing_folds_feats=[fold_1_tst,fold_2_tst,fold_3_tst,fold_4_tst,fold_5_tst]\n",
    "\n",
    "    training_folds_labels=[lab_1_trn,lab_2_trn,lab_3_trn,lab_4_trn,lab_5_trn]\n",
    "\n",
    "    testing_folds_labels=[lab_1_tst,lab_2_tst,lab_3_tst,lab_4_tst,lab_5_tst]\n",
    "    \n",
    "    \n",
    "    model_ProWRAS=[]\n",
    "    for i in range(5):\n",
    "\n",
    "        features = training_folds_feats[i]\n",
    "        labels= training_folds_labels[i]\n",
    "        label_1=np.where(labels == 1)[0]\n",
    "        label_1=list(label_1)\n",
    "        features_1_trn=features[label_1]\n",
    "\n",
    "        label_0=np.where(labels == 0)[0]\n",
    "        label_0=list(label_0)\n",
    "        features_0_trn=features[label_0]\n",
    "\n",
    "        features_trn_fold= np.concatenate((features_1_trn,features_0_trn))\n",
    "        labels_trn_fold= np.concatenate((np.zeros(len(features_1_trn))+1, np.zeros(len(features_0_trn))))\n",
    "\n",
    "        n_neighbours_max=neb\n",
    "        max_levels=5\n",
    "        num_samples_to_generate=int((len(features_0_trn)-len(features_1_trn)))\n",
    "        theta=1\n",
    "        shadow=100\n",
    "        neb_conv=5 #combinations: 1000,5\n",
    "        max_conv=features_x.shape[1] #combinations: features_x.shape[1],2\n",
    "        n_jobs=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        print('Running Classifier \\n')\n",
    "        ProWRAS_train,ProWRAS_labels=ProWRAS_gen(features_trn_fold, labels_trn_fold, max_levels,neb_conv, n_neighbours_max,max_conv, num_samples_to_generate, theta, shadow, sigma_model, n_jobs)\n",
    "        f1_model_ProWRAS,kappa_model_ProWRAS,aps_model_ProWRAS,mat_model_ProWRAS=classifier(ProWRAS_train,ProWRAS_labels,testing_folds_feats[i],testing_folds_labels[i])\n",
    "        model_ProWRAS.append([f1_model_ProWRAS,kappa_model_ProWRAS,aps_model_ProWRAS]) \n",
    "\n",
    "        print('\\n')\n",
    "        print('############################   Fold '+ str(i+1) + ' Finished   ############################\\n\\n')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ## Using oversampling algorithms to generate and test classification models\n",
    "    print('Using oversampling algorithms to generate and test classification models for seed:'+ str(seed_perm)+'\\n')\n",
    "\n",
    "    model=[] \n",
    "    model_SM=[]\n",
    "    model_MOT2TLD=[] \n",
    "    model_ProWSyn=[] \n",
    "    model_CURE=[] \n",
    "    model_SOMO=[] \n",
    "\n",
    "\n",
    "\n",
    "    i=0\n",
    "    while i<5:\n",
    "        SMOTE_feat, SMOTE_labels, SMOTE_feat_polynom_fit_SMOTE, SMOTE_labels_polynom_fit_SMOTE, SMOTE_feat_ProWSyn, SMOTE_labels_ProWSyn, SMOTE_feat_cure, SMOTE_labels_cure, SMOTE_feat_SOMO, SMOTE_labels_SOMO=OVS(training_folds_feats[i],training_folds_labels[i], neb)\n",
    "\n",
    "        \n",
    "        f1_model,kappa_model,aps_model,mat_model=classifier(training_folds_feats[i],training_folds_labels[i],testing_folds_feats[i],testing_folds_labels[i])\n",
    "        model.append([f1_model,kappa_model,aps_model])\n",
    "\n",
    "        f1_model_SMOTE,kappa_model_SMOTE, aps_model_SMOTE, mat_model_SMOTE=classifier(SMOTE_feat,SMOTE_labels,testing_folds_feats[i],testing_folds_labels[i])\n",
    "        model_SM.append([f1_model_SMOTE,kappa_model_SMOTE,aps_model_SMOTE])\n",
    "\n",
    "\n",
    "        f1_model_SMOTE_polynom_fit_SMOTE,kappa_model_SMOTE_polynom_fit_SMOTE, aps_model_SMOTE_polynom_fit_SMOTE, mat_model_SMOTE_polynom_fit_SMOTE=classifier(SMOTE_feat_polynom_fit_SMOTE,SMOTE_labels_polynom_fit_SMOTE,testing_folds_feats[i],testing_folds_labels[i])\n",
    "        model_MOT2TLD.append([f1_model_SMOTE_polynom_fit_SMOTE,kappa_model_SMOTE_polynom_fit_SMOTE, aps_model_SMOTE_polynom_fit_SMOTE])\n",
    "\n",
    "\n",
    "        f1_model_SMOTE_ProWSyn,kappa_model_SMOTE_ProWSyn,aps_model_SMOTE_ProWSyn,mat_model_SMOTE_ProWSyn=classifier(SMOTE_feat_ProWSyn,SMOTE_labels_ProWSyn,testing_folds_feats[i],testing_folds_labels[i])\n",
    "        model_ProWSyn.append([f1_model_SMOTE_ProWSyn,kappa_model_SMOTE_ProWSyn,aps_model_SMOTE_ProWSyn])\n",
    "\n",
    "        f1_model_SMOTE_cure,kappa_model_SMOTE_cure,aps_model_SMOTE_cure,mat_model_SMOTE_cure=classifier(SMOTE_feat_cure, SMOTE_labels_cure,testing_folds_feats[i],testing_folds_labels[i])\n",
    "        model_CURE.append([f1_model_SMOTE_cure,kappa_model_SMOTE_cure,aps_model_SMOTE_cure])\n",
    "\n",
    "\n",
    "        f1_model_SMOTE_SOMO,kappa_model_SMOTE_SOMO,aps_model_SMOTE_SOMO,mat_model_SMOTE_SOMO=classifier(SMOTE_feat_SOMO, SMOTE_labels_SOMO,testing_folds_feats[i],testing_folds_labels[i])\n",
    "        model_SOMO.append([f1_model_SMOTE_SOMO,kappa_model_SMOTE_SOMO,aps_model_SMOTE_SOMO])\n",
    "\n",
    "        print('\\n')\n",
    "        print('############################   Fold '+ str(i+1) + ' Finished   ############################\\n\\n')\n",
    "        i=i+1\n",
    "\n",
    "\n",
    "    ## Using ProWRAS-X to generate and test classification models\n",
    "    print('Using ProWRAS-X to generate and test classification models for seed:'+ str(seed_perm)+'\\n')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ## Summing up results\n",
    "\n",
    "    print('Summing up results for seed:'+ str(seed_perm)+'\\n')\n",
    "\n",
    "    model_m, model_sd= stats(model)\n",
    "\n",
    "    model_SM_m, model_SM_sd=stats(model_SM)\n",
    "\n",
    "    model_MOT2TLD_m, model_MOT2TLD_sd=stats(model_MOT2TLD)\n",
    "\n",
    "    model_ProWSyn_m, model_ProWSyn_sd=stats(model_ProWSyn)\n",
    "    \n",
    "    model_CURE_m, model_CURE_sd=stats(model_CURE)\n",
    "\n",
    "    model_SOMO_m, model_SOMO_sd=stats(model_SOMO)\n",
    "\n",
    "    model_ProWRAS_m, model_ProWRAS_sd=stats(model_ProWRAS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model_F1=np.array(['F1-Score_'+classify, model_m[0], model_SM_m[0], model_MOT2TLD_m[0], model_ProWSyn_m[0], model_CURE_m[0], model_SOMO_m[0], model_ProWRAS_m[0]])\n",
    "    model_kappa=np.array(['Kappa_'+classify, model_m[1], model_SM_m[1], model_MOT2TLD_m[1], model_ProWSyn_m[1], model_CURE_m[1], model_SOMO_m[1], model_ProWRAS_m[1]])\n",
    "   \n",
    "\n",
    "    df=pd.DataFrame(np.array([model_F1,model_kappa]),\n",
    "                       columns=['Performance Measure', 'Baseline', 'SMOTE', 'polynom_fit_SMOTE','ProWSyn','CURE','SOMO','ProWRASX'])\n",
    "\n",
    "    results=df\n",
    "\n",
    "    filename=dataset+'seed_'+str(seed_perm)+'.csv'\n",
    "\n",
    "    results.to_csv(filename, index=False)\n",
    "\n",
    "    print('results ready for seed:'+str(seed_perm)+'\\n')\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "file_list=glob.glob(\"*.csv\")\n",
    "\n",
    "whole_info=[]\n",
    "for i in range(len(file_list)):\n",
    "    arr=np.array(pd.read_csv(file_list[i]))\n",
    "    whole_info.append(arr)\n",
    "whole_info=np.array(whole_info)\n",
    "\n",
    "F1_Scores_LR_mean=np.mean(np.array([whole_info[0][0][1:],whole_info[1][0][1:], whole_info[2][0][1:],whole_info[3][0][1:],whole_info[4][0][1:]]),axis=0)\n",
    "BAs_LR_mean=np.mean(np.array([whole_info[0][1][1:],whole_info[1][1][1:], whole_info[2][1][1:],whole_info[3][1][1:],whole_info[4][1][1:]]),axis=0)\n",
    "agrregate_results_for_dataset=pd.DataFrame(np.array([F1_Scores_LR_mean,BAs_LR_mean]),\n",
    "                       columns=['Baseline', 'SMOTE', 'polynom_fit_SMOTE','ProWSyn','CURE','SOMO','LORASX'], index=np.array(['F1_Scores','Kappa']))\n",
    "filename=file_list[0][:-9]+'agrregate.csv'\n",
    "agrregate_results_for_dataset.to_csv(filename,index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
